# Konfiguracja planszy
grid_width: 8
grid_height: 8

walls:
  - [0, 0]
  - [1, 0]
  - [2, 2]
  - [1, 2]
  - [2, 1]
  - [1, 4]
  - [3, 4]
  - [4, 4]
  - [5, 5]
  - [5, 4]

# Konfiguracja gry
detection_distance: 2 # Zasieg detekcji agenta szukajacego
max_steps: 25 # Maksymalna liczba krokow w grze - po jej przekroczeniu wygrywa chowający się
total_episodes: 100 # Calkowita liczba tur w grze

# Parametry wizualizacji
step_delay: 0.1 # opoznienie dla 1 kroku
terminal_delay: 4 # opoznienie po zakonczeniu tury
visualize: true # Czy wizualizowac gre: false - nie, true - tak

# Zapisywanie danych z symulacji
data_file_path: "data_test.json" # Sciezka do pliku z zapisanymi danymi

# Czy uczyc agentów z wczesniej przeuczonych wartosci Q
hider_from_pretrained: false # false - nie, true - tak
hider_pretrained_file_name: "q_values_hider.json" # jezeli tak - sciezka do json file z wartosciami

seeker_from_pretrained: false # false - nie, true - tak
seeker_pretrained_file_name: "q_values_seeker.json" # jezeli tak - sciezka do json file z wartosciami

# W jakiej nazwie pliku zapisac nauczone wartosci Q
hider_saving_file_path: "q_values_hider.json"
seeker_saving_file_path: "q_values_seeker.json"

# -----------------------------------
# Konfiguracja agenta szukajacego
# -----------------------------------

# Wybor Algorytmu uczenia, dostępne: "QLearning", "Sarsa", "MonteCarlo"
seeker_learning_algorithm: "QLearning" 

# wybor parametrow algorytmu uczenia:
seeker_learning_rate: 0.1 # Dotyczy tylko algorytmów: "QLearning", "Sarsa"
seeker_discount_factor: 0.9
seeker_default_q_value: 100

# Wybor strategii wybierania akcji, dostepne: "EpsilonGreedy", "DecayEpsilonGreedy"
seeker_action_selection_strategy: "EpsilonGreedy"
seeker_initial_epsilon: 1.0
seeker_min_epsilon: 0 # Dotyczy tylko "DecayEpsilonGreedy"

# Wybór definiowania stanu, dostepne: "HearingStateSeeker", "DistanceStateSeeker", 'CompleteKnowledgeState'
seeker_state: "HearingStateSeeker"

# Wybór definiowania nagrod, dostepne: "DurationReward", "WinLoseReward"
seeker_reward: "WinLoseReward"

# -----------------------------------
# Konfiguracja agenta chowajacego
# -----------------------------------

# Wybor Algorytmu uczenia, dostępne: "QLearning", "Sarsa", "MonteCarlo"
hider_learning_algorithm: "Sarsa" 

# wybor parametrow algorytmu uczenia:
hider_learning_rate: 0.05 # Dotyczy tylko algorytmów: "QLearning", "Sarsa"
hider_discount_factor: 0.99
hider_default_q_value: 50

# Wybor strategii wybierania akcji, dostepne: "EpsilonGreedy", "DecayEpsilonGreedy"
hider_action_selection_strategy: "DecayEpsilonGreedy"
hider_initial_epsilon: 1.0
hider_min_epsilon: 0 # Dotyczy tylko "DecayEpsilonGreedy"

# Wybór definiowania stanu, dostepne: "HearingStateHider", "DistanceStateHider", 'CompleteKnowledgeState'
hider_state: "CompleteKnowledgeState"

# Wybór definiowania nagrod, dostepne: "DurationReward", "WinLoseReward"
hider_reward: "DurationReward"
