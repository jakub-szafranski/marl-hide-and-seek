# ==================================================
# GAME ENVIRONMENT CONFIGURATION
# ==================================================

grid_width: 6
grid_height: 6

# Wall positions
walls:
  - [0, 0]
  - [1, 0]
  - [2, 2]
  - [1, 2]
  - [2, 1]
  - [1, 4]
  - [3, 4]
  - [4, 4]
  - [5, 5]
  - [5, 4]
  - [4, 1]

# Game parameters
max_steps: 12  # Maximum steps per episode
total_episodes: 300000  # Total number of training episodes
detection_distance: 2  # Seeker's detection range

# ==================================================
# VISUALIZATION SETTINGS
# ==================================================
visualize: false  # Whether to visualize the game: false - no, true - yes
step_delay: 0.1  # Delay for each step (in seconds)
terminal_delay: 5  # Delay after episode completion (in seconds)

# ==================================================
# DATA MANAGEMENT
# ==================================================

# Simulation data
data_file_path: "simulation_data.json"  # Path for saving simulation data

# Pretrained models
hider_from_pretrained: false  # Load pretrained hider: false - no, true - yes
hider_pretrained_file_name: "q_values_hider.json"  # Path to hider's Q-values if pretrained

seeker_from_pretrained: false  # Load pretrained seeker: false - no, true - yes
seeker_pretrained_file_name: "q_values_seeker.json"  # Path to seeker's Q-values if pretrained

# Output model paths
hider_saving_file_path: "q_values_hider.json"  # Where to save trained hider's Q-values
seeker_saving_file_path: "q_values_seeker.json"  # Where to save trained seeker's Q-values

# ==================================================
# SEEKER AGENT CONFIGURATION
# ==================================================

# Learning parameters
seeker_learning_algorithm: "QLearning"  # Learning algorithm
seeker_learning_rate: 0.1  # Learning rate alpha
seeker_discount_factor: 0.9  # Discount factor gamma
seeker_default_q_value: 5  # Default Q-value for new state-action pairs

# Action selection
seeker_action_selection_strategy: "DecayEpsilonGreedy"  # Strategy for action selection
seeker_initial_epsilon: 1  # Initial exploration rate
seeker_min_epsilon: 0  # Minimum exploration rate

# State representation
seeker_state: "HearingStateSeeker"  # Options: "HearingStateSeeker", "DistanceStateSeeker", "CompleteKnowledgeState"

# Reward function
seeker_reward: "WinLoseReward"  # Options: "DurationReward", "WinLoseReward"

# ==================================================
# HIDER AGENT CONFIGURATION
# ==================================================

# Learning parameters
hider_learning_algorithm: "QLearning"  # Learning algorithm options: "QLearning", "Sarsa", "MonteCarlo"
hider_learning_rate: 0.1  # Learning rate alpha (for QLearning and Sarsa only)
hider_discount_factor: 0.9  # Discount factor gamma
hider_default_q_value: 5  # Default Q-value for new state-action pairs

# Action selection
hider_action_selection_strategy: "DecayEpsilonGreedy"  # Options: "EpsilonGreedy", "DecayEpsilonGreedy"
hider_initial_epsilon: 1  # Initial exploration rate
hider_min_epsilon: 0  # Minimum exploration rate (for DecayEpsilonGreedy only)

# State representation
hider_state: "HearingStateHider"  # Options: "HearingStateHider", "DistanceStateHider", "CompleteKnowledgeState"

# Reward function
hider_reward: "WinLoseReward"  # Options: "DurationReward", "WinLoseReward"