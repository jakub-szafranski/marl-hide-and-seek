# Konfiguracja planszy
grid_width: 7
grid_height: 7

walls:
  - [2, 0]
  - [2, 2]
  - [1, 2]
  - [2, 1]
  - [1, 4]
  - [3, 4]
  - [4, 4]
  - [5, 5]
  - [5, 4]
  - [4, 1]

# Konfiguracja gry
detection_distance: 2 # Zasieg detekcji agenta szukajacego
max_steps: 20 # Maksymalna liczba krokow w grze - po jej przekroczeniu wygrywa chowający się
total_episodes: 100_000 # Calkowita liczba tur w grze

# Parametry wizualizacji
step_delay: 0.1 # opoznienie dla 1 kroku
terminal_delay: 5 # opoznienie po zakonczeniu tury
visualize: true # Czy wizualizowac gre: false - nie, true - tak

# Czy uczyc agentów z wczesniej przeuczonych wartosci Q
from_pretrained: true # false - nie, true - tak
pretrained_file_name: "q_values.json" # jezeli tak - sciezka do json file z wartosciami

# W jakiej nazwie pliku zapisac nauczone wartosci Q
saving_file_path: "q_values.json"

# -----------------------------------
# Konfiguracja agenta szukajacego
# -----------------------------------

# Wybor Algorytmu uczenia, dostępne: "QLearning", "Sarsa", "MonteCarlo"
seeker_learning_algorithm: "QLearning" 

# wybor parametrow algorytmu uczenia:
seeker_learning_rate: 0.05
seeker_discount_factor: 0.99
seeker_default_q_value: 0
seeker_n_step: 5 # Dotyczy tylko algorytmów: "QLearning", "Sarsa"

# Wybor strategii wybierania akcji, dostepne: "EpsilonGreedy", "DecayEpsilonGreedy"
seeker_action_selection_strategy: "EpsilonGreedy"
seeker_initial_epsilon: 0.01
seeker_decay_rate: 0.999 # Dotyczy tylko "DecayEpsilonGreedy"
seeker_min_epsilon: 0.001 # Dotyczy tylko "DecayEpsilonGreedy"

# Wybór definiowania stanu, dostepne: "HearingStateSeeker"
seeker_state: "HearingStateSeeker"

# Wybór definiowania nagrod, dostepne: "DurationReward", "WinLoseReward"
seeker_reward: "DurationReward"

# -----------------------------------
# Konfiguracja agenta chowajacego
# -----------------------------------

# Wybor Algorytmu uczenia, dostępne: "QLearning", "Sarsa", "MonteCarlo"
hider_learning_algorithm: "MonteCarlo" 

# wybor parametrow algorytmu uczenia:
hider_learning_rate: 0.05
hider_discount_factor: 0.99
hider_default_q_value: 0
hider_n_step: 5 # Dotyczy tylko algorytmów: "QLearning", "Sarsa"

# Wybor strategii wybierania akcji, dostepne: "EpsilonGreedy", "DecayEpsilonGreedy"
hider_action_selection_strategy: "DecayEpsilonGreedy"
hider_initial_epsilon: 1.0
hider_decay_rate: 0.999 # Dotyczy tylko "DecayEpsilonGreedy"
hider_min_epsilon: 0.99 # Dotyczy tylko "DecayEpsilonGreedy"

# Wybór definiowania stanu, dostepne: "HearingStateHider"
hider_state: "HearingStateHider"

# Wybór definiowania nagrod, dostepne: "DurationReward", "WinLoseReward"
hider_reward: "DurationReward"